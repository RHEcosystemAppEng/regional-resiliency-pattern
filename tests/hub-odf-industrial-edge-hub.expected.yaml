---
# Source: odf/templates/rbac/label-storage-bundle-sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: label-storage-nodes
  namespace: openshift-storage
  annotations:
    argocd.argoproj.io/sync-wave: "1"
---
# Source: odf/templates/storageclasses.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ocs-storagecluster-cephfs
  annotations:
    description: Provides RWO and RWX Filesystem volumes
    storageclass.kubernetes.io/is-default-class: 'true'
provisioner: openshift-storage.cephfs.csi.ceph.com
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  fsName: ocs-storagecluster-cephfilesystem
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
---
# Source: odf/templates/storageclasses.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ocs-storagecluster-ceph-rgw
  annotations:
    description: Provides Object Bucket Claims (OBCs)
provisioner: openshift-storage.ceph.rook.io/bucket
parameters:
  objectStoreName: ocs-storagecluster-cephobjectstore
  objectStoreNamespace: openshift-storage
  region: us-east-1
reclaimPolicy: Delete
volumeBindingMode: Immediate
---
# Source: odf/templates/storageclasses.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ocs-storagecluster-ceph-rbd
  annotations:
    description: 'Provides RWO Filesystem volumes, and RWO and RWX Block volumes'
provisioner: openshift-storage.rbd.csi.ceph.com
parameters:
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  imageFormat: '2'
  clusterID: openshift-storage
  imageFeatures: 'layering,deep-flatten,exclusive-lock,object-map,fast-diff'
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  pool: ocs-storagecluster-cephblockpool
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
---
# Source: odf/templates/rbac/label-storage-bundle-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  name: label-storage-nodes
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
      - list
      - patch
      - update
---
# Source: odf/templates/rbac/label-storage-bundle-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: label-storage-nodes
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: label-storage-nodes
subjects:
  - kind: ServiceAccount
    name: label-storage-nodes
    namespace: openshift-storage
---
# Source: odf/templates/rbac/quay-noobaa-deploy-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: quay-noobaa-deploy-role
  annotations:
    argocd.argoproj.io/sync-wave: "2"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: quay-admin-role
subjects:
  - kind: ServiceAccount
    name: quay-admin-sa
    namespace: openshift-storage
---
# Source: odf/templates/label-storage-nodes-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "3"
  name: label-storage-nodes-bundle
spec:
  template:
    spec:
      containers:
        - image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest
          command:
            - /bin/bash
            - -c
            - |
              #!/usr/bin/env bash
              set -e
              if ! oc get nodes -l cluster.ocs.openshift.io/openshift-storage= ; then
                 echo "Worker nodes already labeled for storage"
                exit 0
              else
                # Wait for central to be ready
                attempt_counter=0
                max_attempts=20
                echo "Labeling all worker nodes for storage"
                oc label node -l node-role.kubernetes.io/worker= cluster.ocs.openshift.io/openshift-storage=
                echo "Worker nodes labeled for storage"
              fi
          imagePullPolicy: Always
          name: label-storage-nodes-bundle
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      serviceAccount: label-storage-nodes
      serviceAccountName: label-storage-nodes
      terminationGracePeriodSeconds: 30
---
# Source: odf/templates/storagecluster.yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: ocs-storagecluster-cephcluster
  namespace: openshift-storage
  labels:
    app: ocs-storagecluster
spec:
  security:
    keyRotation:
      enabled: false
    kms: {}
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: cluster.ocs.openshift.io/openshift-storage
                  operator: Exists
      tolerations:
        - effect: NoSchedule
          key: node.ocs.openshift.io/storage
          operator: Equal
          value: 'true'
    arbiter:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
          operator: Exists
    mon:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: cluster.ocs.openshift.io/openshift-storage
                  operator: Exists
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - rook-ceph-mon
            topologyKey: kubernetes.io/hostname
  crashCollector: {}
  monitoring:
    enabled: true
  logCollector:
    enabled: true
    maxLogSize: 500Mi
    periodicity: daily
  external: {}
  healthCheck:
    daemonHealth:
      mon: {}
      osd: {}
      status: {}
  mon:
    count: 3
  network:
    connections:
      encryption: {}
      requireMsgr2: true
    multiClusterService: {}
  dataDirHostPath: /var/lib/rook
  continueUpgradeAfterChecksEvenIfNotHealthy: true
  priorityClassNames:
    mgr: system-node-critical
    mon: system-node-critical
    osd: system-node-critical
  dashboard: {}
  cleanupPolicy:
    sanitizeDisks: {}
  disruptionManagement:
    machineDisruptionBudgetNamespace: openshift-machine-api
    managePodBudgets: true
  mgr:
    count: 1
    modules:
      - enabled: true
        name: pg_autoscaler
      - enabled: true
        name: balancer
  storage:
    storageClassDeviceSets:
      - count: 3
        name: ocs-deviceset-gp3-csi
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: cluster.ocs.openshift.io/openshift-storage
                      operator: Exists
          tolerations:
            - effect: NoSchedule
              key: node.ocs.openshift.io/storage
              operator: Equal
              value: 'true'
          topologySpreadConstraints:
            - labelSelector:
                matchExpressions:
                  - key: ceph.rook.io/pvc
                    operator: Exists
              maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
        preparePlacement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: cluster.ocs.openshift.io/openshift-storage
                      operator: Exists
          tolerations:
            - effect: NoSchedule
              key: node.ocs.openshift.io/storage
              operator: Equal
              value: 'true'
          topologySpreadConstraints:
            - labelSelector:
                matchExpressions:
                  - key: ceph.rook.io/pvc
                    operator: Exists
              maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
        resources:
          limits:
            cpu: '2'
            memory: 5Gi
          requests:
            cpu: '1'
            memory: 5Gi
        volumeClaimTemplates:
          - metadata:
              annotations:
                crushDeviceClass: ''
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: '1'
              storageClassName: gp3-csi
              volumeMode: Block
            status: {}
  cephVersion:
    image: >-
      registry.redhat.io/rhceph/rhceph-6-rhel9@sha256:9fe3736781784da48ea5605370a0d497087162105a42e19d1fbada54757e3f44
  labels:
    monitoring:
      rook.io/managedBy: ocs-storagecluster
---
# Source: odf/templates/policy-odf-status.yaml
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  annotations:
    policy.open-cluster-management.io/categories: SI System and Information Integrity
    policy.open-cluster-management.io/controls: SI-7 Software Firmware and Information
      Integrity
    policy.open-cluster-management.io/standards: NIST SP 800-53
    argocd.argoproj.io/compare-options: IgnoreExtraneous
  labels:
    open-cluster-management.io/policy-set: openshift-plus
  name: policy-odf-status
  namespace: policies
spec:
  disabled: false
  policy-templates:
  - objectDefinition:
      apiVersion: policy.open-cluster-management.io/v1
      kind: ConfigurationPolicy
      metadata:
        name: policy-odf-status
      spec:
        object-templates:
        - complianceType: musthave
          objectDefinition:
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: noobaa-operator
              namespace: openshift-storage
            status:
              conditions:
              - status: "True"
                type: Available
        - complianceType: musthave
          objectDefinition:
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: ocs-operator
              namespace: openshift-storage
            status:
              conditions:
              - status: "True"
                type: Available
        - complianceType: musthave
          objectDefinition:
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: odf-operator-controller-manager
              namespace: openshift-storage
            status:
              conditions:
              - status: "True"
                type: Available
        remediationAction: inform
        severity: medium
---
# Source: odf/templates/storagecluster.yaml
apiVersion: ocs.openshift.io/v1
kind: StorageCluster
metadata:
  annotations:
    uninstall.ocs.openshift.io/cleanup-policy: delete
    uninstall.ocs.openshift.io/mode: graceful
  finalizers:
  - storagecluster.ocs.openshift.io
  name: ocs-storagecluster
  namespace: openshift-storage
spec:
  arbiter: {}
  encryption:
    kms: {}
  externalStorage: {}
  managedResources:
    cephBlockPools: {}
    cephCluster: {}
    cephConfig: {}
    cephDashboard: {}
    cephFilesystems: {}
    cephObjectStoreUsers: {}
    cephObjectStores: {}
  mirroring: {}
  multiCloudGateway:
    dbStorageClassName: default-rwo
    reconcileStrategy: standalone
  storageDeviceSets:
    - config: {}
      count: 1
      dataPVCTemplate:
        metadata: {}
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 512Gi
          storageClassName: gp3-csi
          volumeMode: Block
      name: ocs-deviceset-gp3-csi
      placement: {}
      portable: true
      preparePlacement: {}
      replica: 3
